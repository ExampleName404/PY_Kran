import gymnasium as gym
from gymnasium import spaces
import pybullet as p
import pybullet_data
import numpy as np
import cv2
import torch
import torch.nn as nn
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import BaseCallback
import os


class RobotArmEnv(gym.Env):
    """–°—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é"""
    
    def __init__(self, render_mode=None, use_gui=False, use_stereo=True):
        super().__init__()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–º–µ—Ä—ã
        self.img_width = 64
        self.img_height = 64
        self.use_grayscale = True  # –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
        self.use_stereo = use_stereo  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–µ –∫–∞–º–µ—Ä—ã –¥–ª—è —Å—Ç–µ—Ä–µ–æ–∑—Ä–µ–Ω–∏—è
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∏–º—É–ª—è—Ü–∏–∏
        self.frame_skip = 4  # –ü–æ–≤—Ç–æ—Ä—è—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ N —Ä–∞–∑
        self.max_steps = 100
        self.current_step = 0
        
        # Action space: —Å–º–µ—â–µ–Ω–∏–µ —Å—Ö–≤–∞—Ç–∞ (dx, dy, dz)
        self.action_space = spaces.Box(
            low=-0.05, 
            high=0.05, 
            shape=(3,), 
            dtype=np.float32
        )
        
        # Observation space: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + —É–≥–ª—ã –¥–∂–æ–∏–Ω—Ç–æ–≤
        n_channels = 1 if self.use_grayscale else 3
        # –ï—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ–∑—Ä–µ–Ω–∏–µ - —É–¥–≤–∞–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤
        if self.use_stereo:
            n_channels *= 2
        
        self.observation_space = spaces.Dict({
            'image': spaces.Box(
                low=0, high=255,
                shape=(self.img_height, self.img_width, n_channels),
                dtype=np.uint8
            ),
            'joints': spaces.Box(
                low=-np.pi, high=np.pi,
                shape=(7,),
                dtype=np.float32
            )
        })
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyBullet
        self.use_gui = use_gui
        if self.use_gui:
            self.physics_client = p.connect(p.GUI)
        else:
            self.physics_client = p.connect(p.DIRECT)
        
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.81)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ü–µ–Ω—ã
        self.plane_id = None
        self.robot_id = None
        self.object_id = None
        self.target_pos = None
        
        self._setup_scene()
    
    def _setup_scene(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ü–µ–Ω—ã: —Å—Ç–æ–ª, —Ä–æ–±–æ—Ç, –æ–±—ä–µ–∫—Ç"""
        # –ü–ª–æ—Å–∫–æ—Å—Ç—å (—Å—Ç–æ–ª) - –¥–µ–ª–∞–µ–º —Ç–µ–º–Ω–æ–π –¥–ª—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ —Å —è—Ä–∫–∏–º –æ–±—ä–µ–∫—Ç–æ–º
        self.plane_id = p.loadURDF("plane.urdf")
        # –ò–∑–º–µ–Ω—è–µ–º —Ü–≤–µ—Ç –ø–ª–æ—Å–∫–æ—Å—Ç–∏ –Ω–∞ —Ç–µ–º–Ω—ã–π
        p.changeVisualShape(self.plane_id, -1, rgbaColor=[0.2, 0.2, 0.2, 1])
        
        # –†–æ–±–æ—Ç Kuka IIWA
        self.robot_id = p.loadURDF(
            "kuka_iiwa/model.urdf",
            basePosition=[0, 0, 0],
            useFixedBase=True
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–æ–±–æ—Ç–∞
        self.num_joints = p.getNumJoints(self.robot_id)
        self.ee_index = 6  # End-effector link index
        
        # –ù–∞—á–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–æ–±–æ—Ç–∞
        self.initial_joint_positions = [0, 0.5, 0, -1.5, 0, 1.0, 0]
        for i in range(len(self.initial_joint_positions)):
            p.resetJointState(self.robot_id, i, self.initial_joint_positions[i])
        
        # –¶–µ–ª–µ–≤–æ–π –æ–±—ä–µ–∫—Ç (–∫—É–±)
        self.object_id = None  # –°–æ–∑–¥–∞–µ—Ç—Å—è –≤ reset()
    
    def _get_camera_image(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –∫–∞–º–µ—Ä—ã (Eye-to-hand)"""
        if self.use_stereo:
            # –î–≤–µ –∫–∞–º–µ—Ä—ã –¥–ª—è —Å—Ç–µ—Ä–µ–æ–∑—Ä–µ–Ω–∏—è (–ª–µ–≤–∞—è –∏ –ø—Ä–∞–≤–∞—è)
            images = []
            camera_positions = [
                [0.45, -0.1, 1.0],  # –õ–µ–≤–∞—è –∫–∞–º–µ—Ä–∞
                [0.45, 0.1, 1.0]    # –ü—Ä–∞–≤–∞—è –∫–∞–º–µ—Ä–∞
            ]
            
            for cam_pos in camera_positions:
                view_matrix = p.computeViewMatrix(
                    cameraEyePosition=cam_pos,
                    cameraTargetPosition=[0.5, 0, 0.3],
                    cameraUpVector=[0, 0, 1]
                )
                
                proj_matrix = p.computeProjectionMatrixFOV(
                    fov=60,
                    aspect=1.0,
                    nearVal=0.1,
                    farVal=3.0
                )
                
                img = p.getCameraImage(
                    self.img_width,
                    self.img_height,
                    view_matrix,
                    proj_matrix,
                    renderer=p.ER_TINY_RENDERER
                )
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                rgb_array = np.array(img[2], dtype=np.uint8)
                rgb_array = rgb_array.reshape((self.img_height, self.img_width, 4))[:, :, :3]
                
                if self.use_grayscale:
                    gray = np.dot(rgb_array, [0.299, 0.587, 0.114])
                    images.append(gray.astype(np.uint8))
                else:
                    images.append(rgb_array.astype(np.uint8))
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∫–∞–Ω–∞–ª–∞–º
            if self.use_grayscale:
                return np.stack(images, axis=-1)  # (H, W, 2)
            else:
                return np.concatenate(images, axis=-1)  # (H, W, 6)
        else:
            # –û–¥–Ω–∞ –∫–∞–º–µ—Ä–∞ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
            view_matrix = p.computeViewMatrix(
                cameraEyePosition=[0.5, 0, 1.0],
                cameraTargetPosition=[0.5, 0, 0.3],
                cameraUpVector=[0, 0, 1]
            )
            
            proj_matrix = p.computeProjectionMatrixFOV(
                fov=60,
                aspect=1.0,
                nearVal=0.1,
                farVal=3.0
            )
            
            img = p.getCameraImage(
                self.img_width,
                self.img_height,
                view_matrix,
                proj_matrix,
                renderer=p.ER_TINY_RENDERER
            )
            
            rgb_array = np.array(img[2], dtype=np.uint8)
            rgb_array = rgb_array.reshape((self.img_height, self.img_width, 4))[:, :, :3]
            
            if self.use_grayscale:
                gray = np.dot(rgb_array, [0.299, 0.587, 0.114])
                return gray.astype(np.uint8).reshape(self.img_height, self.img_width, 1)
            else:
                return rgb_array.astype(np.uint8)
    
    def _get_joint_states(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É–≥–ª–æ–≤ –¥–∂–æ–∏–Ω—Ç–æ–≤ —Ä–æ–±–æ—Ç–∞ (–ø—Ä–æ–ø—Ä–∏–æ—Ü–µ–ø—Ü–∏—è)"""
        joint_states = []
        for i in range(7):
            state = p.getJointState(self.robot_id, i)
            joint_states.append(state[0])  # –£–≥–æ–ª
        return np.array(joint_states, dtype=np.float32)
    
    def _compute_reward(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã"""
        # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è —Å—Ö–≤–∞—Ç–∞
        ee_state = p.getLinkState(self.robot_id, self.ee_index)
        ee_pos = np.array(ee_state[0])
        
        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–ª–∏
        distance = np.linalg.norm(ee_pos - self.target_pos)
        
        # Dense reward: —à—Ç—Ä–∞—Ñ –∑–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        reward = -2.0 * distance
        
        # Sparse reward: –±–æ–Ω—É—Å –∑–∞ –∫–∞—Å–∞–Ω–∏–µ
        contacts = p.getContactPoints(self.robot_id, self.object_id)
        if len(contacts) > 0:
            reward += 10.0
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –∫–∞–∂–¥—ã–π —à–∞–≥ (—Å—Ç–∏–º—É–ª –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä–æ)
        reward -= 0.01
        
        # –¢–µ—Ä–º–∏–Ω–∞—Ü–∏—è –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
        done = distance < 0.05
        if done:
            reward += 50.0  # –ë–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ —É—Å–ø–µ—Ö
        
        # –¢–µ—Ä–º–∏–Ω–∞—Ü–∏—è –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ —à–∞–≥–æ–≤
        truncated = self.current_step >= self.max_steps
        
        return reward, done, truncated
    
    def reset(self, seed=None, options=None):
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã"""
        super().reset(seed=seed)
        
        self.current_step = 0
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
        if self.object_id is not None:
            p.removeBody(self.object_id)
        
        # –°–±—Ä–æ—Å —Ä–æ–±–æ—Ç–∞ –≤ –Ω–∞—á–∞–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
        for i in range(len(self.initial_joint_positions)):
            p.resetJointState(self.robot_id, i, self.initial_joint_positions[i])
        
        # –°–ª—É—á–∞–π–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –æ–±—ä–µ–∫—Ç–∞ –≤ —Ä–∞–±–æ—á–µ–π –∑–æ–Ω–µ
        x = np.random.uniform(0.3, 0.7)
        y = np.random.uniform(-0.3, 0.3)
        z = 0.5
        self.target_pos = np.array([x, y, z])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ (–∫—É–±) - –Ø–†–ö–ò–ô –ö–û–ù–¢–†–ê–°–¢–ù–´–ô —Ü–≤–µ—Ç –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
        collision_shape = p.createCollisionShape(p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.03])
        visual_shape = p.createVisualShape(
            p.GEOM_BOX,
            halfExtents=[0.03, 0.03, 0.03],
            rgbaColor=[1, 1, 0, 1]  # –ñ–ï–õ–¢–´–ô —Ü–≤–µ—Ç - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–µ–Ω –Ω–∞ —Ç–µ–º–Ω–æ–º —Ñ–æ–Ω–µ
        )
        self.object_id = p.createMultiBody(
            baseMass=0.1,
            baseCollisionShapeIndex=collision_shape,
            baseVisualShapeIndex=visual_shape,
            basePosition=self.target_pos
        )
        
        # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è —Å—Ü–µ–Ω—ã
        for _ in range(10):
            p.stepSimulation()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
        observation = {
            'image': self._get_camera_image(),
            'joints': self._get_joint_states()
        }
        
        return observation, {}
    
    def step(self, action):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è"""
        self.current_step += 1
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è: —Å–º–µ—â–µ–Ω–∏–µ end-effector
        ee_state = p.getLinkState(self.robot_id, self.ee_index)
        current_pos = np.array(ee_state[0])
        target_pos = current_pos + action
        
        # Inverse kinematics –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É–≥–ª–æ–≤ –¥–∂–æ–∏–Ω—Ç–æ–≤
        joint_poses = p.calculateInverseKinematics(
            self.robot_id,
            self.ee_index,
            target_pos
        )
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–≥–ª–æ–≤ –∫ –¥–∂–æ–∏–Ω—Ç–∞–º
        for i in range(7):
            p.setJointMotorControl2(
                self.robot_id,
                i,
                p.POSITION_CONTROL,
                targetPosition=joint_poses[i],
                force=200
            )
        
        # Frame skipping: –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ N —Ä–∞–∑
        for _ in range(self.frame_skip):
            p.stepSimulation()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
        observation = {
            'image': self._get_camera_image(),
            'joints': self._get_joint_states()
        }
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã
        reward, done, truncated = self._compute_reward()
        
        return observation, reward, done, truncated, {}
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å—Ä–µ–¥—ã"""
        p.disconnect()


class NatureCNN(BaseFeaturesExtractor):
    """–õ–µ–≥–∫–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–í–∞—Ä–∏–∞–Ω—Ç –ê)"""
    
    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 256):
        super().__init__(observation_space, features_dim)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # VecTransposeImage –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É –Ω–∞ (C, H, W) –≤–º–µ—Å—Ç–æ (H, W, C)
        img_shape = observation_space['image'].shape
        if img_shape[0] in [1, 2, 3, 4, 6] and img_shape[1] == 64 and img_shape[2] == 64:
            # –§–æ—Ä–º–∞—Ç (C, H, W) –ø–æ—Å–ª–µ VecTransposeImage
            n_input_channels = img_shape[0]
        else:
            # –§–æ—Ä–º–∞—Ç (H, W, C) - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π
            n_input_channels = img_shape[2]
        
        # –õ–µ–≥–∫–∞—è CNN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å –º–∞–ª—ã–º–∏ —è–¥—Ä–∞–º–∏ (3x3)
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),  # 64x64 -> 32x32
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 32x32 -> 16x16
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # 16x16 -> 16x16
            nn.ReLU(),
            nn.Flatten(),
        )
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã—Ö–æ–¥–∞ CNN
        with torch.no_grad():
            sample_input = torch.zeros(1, n_input_channels, 64, 64)
            n_flatten = self.cnn(sample_input).shape[1]
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —É–≥–ª–æ–≤ –¥–∂–æ–∏–Ω—Ç–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –∏–∑ observation_space
        # VecFrameStack –º–æ–∂–µ—Ç —Å—Ç–µ–∫–∞—Ç—å joints, –ø–æ—ç—Ç–æ–º—É –±–µ—Ä–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        n_joints = observation_space['joints'].shape[0]
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.linear = nn.Sequential(
            nn.Linear(n_flatten + n_joints, features_dim),
            nn.ReLU(),
        )
        
        self.n_joints = n_joints
    
    def forward(self, observations):
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ (B, H, W, C) –≤ (B, C, H, W)
        image = observations['image'].float() / 255.0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if image.shape[1] == 2 and image.shape[2] == 64 and image.shape[3] == 64:
            # –£–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ (B, C, H, W) - VecTransposeImage —Å—Ä–∞–±–æ—Ç–∞–ª
            pass
        elif image.shape[1] == 64 and image.shape[2] == 64 and image.shape[3] == 2:
            # –§–æ—Ä–º–∞—Ç (B, H, W, C) - –Ω—É–∂–Ω–æ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞—Ç—å
            image = image.permute(0, 3, 1, 2)
        else:
            # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            if len(image.shape) == 4 and image.shape[-1] in [1, 2, 3, 4, 6]:
                image = image.permute(0, 3, 1, 2)
        
        # –ü—Ä–æ–ø—É—Å–∫ —á–µ—Ä–µ–∑ CNN
        cnn_features = self.cnn(image)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —É–≥–ª–∞–º–∏ –¥–∂–æ–∏–Ω—Ç–æ–≤
        joints = observations['joints']
        combined = torch.cat([cnn_features, joints], dim=1)
        
        return self.linear(combined)


class TensorboardCallback(BaseCallback):
    """Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ TensorBoard"""
    
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.current_reward = 0
        self.current_length = 0
    
    def _on_step(self):
        self.current_reward += self.locals['rewards'][0]
        self.current_length += 1
        
        if self.locals['dones'][0]:
            self.episode_rewards.append(self.current_reward)
            self.episode_lengths.append(self.current_length)
            
            self.logger.record('rollout/ep_rew_mean', np.mean(self.episode_rewards[-100:]))
            self.logger.record('rollout/ep_len_mean', np.mean(self.episode_lengths[-100:]))
            
            self.current_reward = 0
            self.current_length = 0
        
        return True


def train_robot():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print("ü§ñ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è Visual RL...")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
    print("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã...")
    print("üëÅÔ∏èüëÅÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –°–¢–ï–†–ï–û–ó–†–ï–ù–ò–ï (–¥–≤–µ –∫–∞–º–µ—Ä—ã) –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –≥–ª—É–±–∏–Ω—ã")
    env = RobotArmEnv(use_gui=False, use_stereo=True)  # use_stereo=True –¥–ª—è –¥–≤—É—Ö –∫–∞–º–µ—Ä
    env = DummyVecEnv([lambda: env])
    
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: VecFrameStack –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å Dict observation space
    # –°—Ç–µ—Ä–µ–æ–∑—Ä–µ–Ω–∏–µ —É–∂–µ –¥–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≥–ª—É–±–∏–Ω–µ
    print("‚ÑπÔ∏è  Frame stacking –æ—Ç–∫–ª—é—á–µ–Ω (–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å Dict obs space)")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")
    if device == "cpu":
        print("‚ö†Ô∏è  GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ PPO —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π CNN
    print("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ PPO —Å NatureCNN...")
    policy_kwargs = dict(
        features_extractor_class=NatureCNN,
        features_extractor_kwargs=dict(features_dim=256),
        normalize_images=False,  # –û—Ç–∫–ª—é—á–∞–µ–º VecTransposeImage –¥–ª—è Dict observation space
    )
    
    model = PPO(
        "MultiInputPolicy",
        env,
        policy_kwargs=policy_kwargs,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        tensorboard_log="./logs/visual_rl/",
        device=device,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    )
    
    print("\n‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é!")
    print(f"üìä Observation space: {env.observation_space}")
    print(f"üéÆ Action space: {env.action_space}")
    print("\n" + "=" * 60)
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    print("=" * 60)
    print("\nüí° –°–æ–≤–µ—Ç—ã:")
    print("  - –û—Ç–∫—Ä–æ–π—Ç–µ TensorBoard: tensorboard --logdir ./logs/visual_rl/")
    print("  - –û–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç 30-60 –º–∏–Ω—É—Ç –Ω–∞ CPU")
    print("  - –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ use_gui=True –≤ RobotArmEnv")
    print("\n" + "=" * 60 + "\n")
    
    # –û–±—É—á–µ–Ω–∏–µ
    callback = TensorboardCallback()
    model.learn(
        total_timesteps=500_000,
        callback=callback,
        progress_bar=True
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    os.makedirs("./models", exist_ok=True)
    model.save("./models/visual_rl_robot")
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: ./models/visual_rl_robot.zip")
    print(f"üìä –õ–æ–≥–∏ TensorBoard: ./logs/visual_rl/")
    
    env.close()


def test_robot():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
    env = RobotArmEnv(use_gui=True, use_stereo=True)
    env = DummyVecEnv([lambda: env])
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = PPO.load("./models/visual_rl_robot", env=env)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 10 —ç–ø–∏–∑–æ–¥–∞—Ö
    for episode in range(10):
        obs = env.reset()
        done = False
        episode_reward = 0
        steps = 0
        
        print(f"\nüìç –≠–ø–∏–∑–æ–¥ {episode + 1}/10")
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            episode_reward += reward[0]
            steps += 1
        
        print(f"   –ù–∞–≥—Ä–∞–¥–∞: {episode_reward:.2f}, –®–∞–≥–æ–≤: {steps}")
    
    env.close()
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_robot()
    else:
        # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        train_robot()